# ì˜ìƒì„ ì°¸ê³ í•´ì„œ ë§Œë“¤ê¸° ì‹œì‘ https://www.youtube.com/watch?v=ZVmLe3odQvc

import streamlit as st
from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification, GPT2LMHeadModel
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import torch
from peft import LoraConfig, TaskType, get_peft_model

# ---------------------------------------------------------------- #
#%% Streamlit ì•±

st.set_page_config(page_title="ê³µê°ì±—", page_icon="ğŸ’¬")
st.title("ğŸ’¬ê³µê° ì±—ë´‡ì´ê±¸ë‘~")

# ì„¸ì…˜ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì„¸ì…˜ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state.store = dict()
    
with st.sidebar:
    session_id = st.text_input("ì´ë¦„ì„ ì•Œë ¤ì£¼ì„¸ìš”", value="ex) ì›Œë¼ë°¸")
    
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state.messages = []
        st.session_state.store = dict()

# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
def print_message():
    if "messages" in st.session_state and len(st.session_state.messages) > 0:
        for chat_message in st.session_state.messages:
            st.chat_message(chat_message.role).write(chat_message.content)
print_message()

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in st.session_state.store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        st.session_state.store[session_id] = ChatMessageHistory() # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ê³  storeì— ì €ì¥
    session_history = st.session_state.store[session_id]

    # ë©”ì‹œì§€ ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ nìë¥¼ ë„˜ìœ¼ë©´ ê°€ì¥ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ì œê±°
    total_length = sum(len(m.content) for m in session_history.messages)
    while total_length > 500 and session_history.messages:
        total_length -= len(session_history.messages[0].content)
        session_history.messages.pop(0)

    return session_history  # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ChatMessageHistory ê°ì²´ ë°˜í™˜

# ---------------------------------------------------------------- #
#%% ì¶”ë¡  ëª¨ë¸ ì¤€ë¹„

# ë³€ìˆ˜
# ê·¸ ì™¸ ì¶”ê°€ ê°€ëŠ¥ ì¸ì https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig
# threshold=0.55 # float, 0~1
# max_length = 45 # int, default=20
# min_new_tokens = 3 # int
# use_cache = True # bool, default=True
# repetition_penalty = 4.0 # float, default=1.0
# do_sample = False # bool, default=False
# num_beams = 2 # int, default=1
# temperature = 2.0 # float, default=1.0
# top_k = 50 # int, default=50
# top_p = 0.9 # float, default=1.0

# ë³€ìˆ˜ë“¤ì„ sidebarì—ì„œ ì…ë ¥ë°›ê¸°
with st.sidebar:
    threshold = st.slider("Threshold", 0.0, 1.0, 0.55)
    max_length = st.number_input("Max Length", value=45, step=1)
    min_new_tokens = st.number_input("Min New Tokens", value=3, step=1)
    use_cache = st.checkbox("Use Cache", value=True)
    repetition_penalty = st.number_input("Repetition Penalty", value=4.0, step=0.1)
    do_sample = st.checkbox("Do Sample", value=False)
    num_beams = st.number_input("Num Beams", value=2, step=1)
    temperature = st.number_input("Temperature", value=2.0, step=0.1)
    top_k = st.number_input("Top K", value=50, step=1)
    top_p = st.number_input("Top P", value=0.9, step=0.1)

cls_peft_config = LoraConfig(
    task_type="SEQ_CLS",
    inference_mode=True,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none")

gen_peft_config = LoraConfig(
    task_type="CAUSAL_LM",
    inference_mode=True,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none")

Q_TKN = "<Q>"
A_TKN = "<A>"
BOS = '</s>'
EOS = '</s>'
UNK = '<unk>'
MASK = '<unused0>'
SENT = '<unused1>'
PAD = '<pad>'

# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
cls_path = './models/kogpt2-classification-lora'
cls_model = AutoModelForSequenceClassification.from_pretrained(
      cls_path,
      num_labels=5,
      problem_type="multi_label_classification"
)
trained_cls_model = get_peft_model(cls_model, cls_peft_config)
trained_cls_tokenizer = PreTrainedTokenizerFast.from_pretrained(cls_path,
                          bos_token=BOS, eos_token=EOS, unk_token=UNK,
                          pad_token=PAD, mask_token=MASK)

def predict_listener_empathy(input_text, model, tokenizer, threshold=threshold):
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    model.eval()

    # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)

    # ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ë¡œì§“(logits)ì„ ì–»ìŒ
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # ë¡œì§“ì— ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
    probabilities = torch.sigmoid(logits)
    # ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ì§„í™”
    predictions = (probabilities > threshold).int()

    # ë ˆì´ë¸” ë””ì½”ë”©
    label_classes = ['ì¡°ì–¸', 'ê²©ë ¤', 'ìœ„ë¡œ', 'ë™ì¡°', '']
    num_classes = 5
    predicted_labels = [label_classes[i] for i in range(num_classes) if predictions[0][i] == 1]

    return predicted_labels

# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
gen_path = './models/kogpt2-chatbot'
gen_model = GPT2LMHeadModel.from_pretrained(gen_path)

trained_gen_model = get_peft_model(gen_model, gen_peft_config)
trained_gen_tokenizer = PreTrainedTokenizerFast.from_pretrained(gen_path,
                          bos_token=BOS, eos_token=EOS, unk_token=UNK,
                          pad_token=PAD, mask_token=MASK)

def predict_answer(predicted_labels, input_text, model, tokenizer,
                   max_length, min_new_tokens, use_cache,
                   repetition_penalty, do_sample, num_beams,
                   temperature, top_k, top_p):
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    model.eval()
    # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    empathy = ' ,'.join(map(str, predicted_labels))
    inputs = Q_TKN + input_text + SENT + empathy + A_TKN
    input_ids = tokenizer.encode(tokenizer.bos_token + inputs + tokenizer.eos_token, return_tensors='pt')

    # ëª¨ë¸ ì¶”ë¡ 
    outputs = model.generate(input_ids,
                             max_length=max_length, min_new_tokens=min_new_tokens, use_cache=use_cache,
                             repetition_penalty=repetition_penalty, do_sample=do_sample, num_beams=num_beams,
                             temperature=temperature, top_k=top_k, top_p=top_p, early_stopping=True)
    output_text = trained_gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    output_text = output_text.split(A_TKN)[1]

    return output_text

# ---------------------------------------------------------------- #
#%% ì±—ë´‡ ëŒ€í™” ë¡œì§

# if user_input := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
#     # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
#     st.chat_message("user").write(f"{user_input}")
#     st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    
#     # ë¶„ë¥˜ ê²°ê³¼ ì¶”ë¡ 
#     # threshold ì˜ ì„¤ì •í•´ì•¼
#     predicted_labels = predict_listener_empathy(user_input, trained_cls_model, trained_cls_tokenizer, threshold)

#     # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„±
#     # session_history = get_session_history(session_id)
#     # history_messages = [ChatMessage(role=m.role, content=m.content) for m in session_history.messages]
#     # history_messages.append(ChatMessage(role="user", content=user_input))
#     # print("history_messages", history_messages)

#     # ì‘ë‹µ ìƒì„±
#     msg = predict_answer(predicted_labels, user_input, trained_gen_model, trained_gen_tokenizer,
#                    max_length, min_new_tokens, use_cache,
#                    repetition_penalty, do_sample, num_beams,
#                    temperature, top_k, top_p)
#     print("msg", msg)

#     # AIì˜ ë‹µë³€
#     with st.chat_message("assistant"):
#         st.write(msg)
#         st.session_state.messages.append(ChatMessage(role="assistant", content=msg))
#         # ì„¸ì…˜ ê¸°ë¡ì— AIì˜ ì‘ë‹µ ì¶”ê°€
#         # session_history.add_message(ChatMessage(role="assistant", content=msg))
#         # print("history_messages", session_history)

# ì±—ë´‡ ëŒ€í™” ë¡œì§ì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ ë¶€ë¶„
if user_input := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    
    # ë¶„ë¥˜ ê²°ê³¼ ì¶”ë¡ 
    predicted_labels = predict_listener_empathy(user_input, trained_cls_model, trained_cls_tokenizer, threshold)

    # ì‘ë‹µ ìƒì„±
    msg = predict_answer(predicted_labels, user_input, trained_gen_model, trained_gen_tokenizer,
                         max_length, min_new_tokens, use_cache,
                         repetition_penalty, do_sample, num_beams,
                         temperature, top_k, top_p)

    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        st.write(msg)
        st.session_state.messages.append(ChatMessage(role="assistant", content=msg))
    