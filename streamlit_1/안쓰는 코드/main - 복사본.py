# ì˜ìƒì„ ì°¸ê³ í•´ì„œ ë§Œë“¤ê¸° ì‹œì‘ https://www.youtube.com/watch?v=ZVmLe3odQvc

import streamlit as st
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.callbacks.base import BaseCallbackHandler


st.set_page_config(page_title="ê³µê°ì±—", page_icon="ğŸ’¬",
                   layout="wide", initial_sidebar_state="expanded")
st.title("ğŸ’¬ê³µê° ì±—ë´‡ì´ê±¸ë‘~")

# ì„¸ì…˜ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì„¸ì…˜ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state.store = dict()

# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# KoGPT2 ëª¨ë¸ ë¡œë“œ
trained_model = GPT2LMHeadModel.from_pretrained('./kogpt2-chatbot')
trained_tokenizer = PreTrainedTokenizerFast.from_pretrained('./kogpt2-chatbot')

# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
def print_message():
    if "messages" in st.session_state and len(st.session_state.messages) > 0:
        for chat_message in st.session_state.messages:
            st.chat_message(chat_message.role).write(chat_message.content)
print_message()

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in st.session_state.store: # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ê³  storeì— ì €ì¥
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id] # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ChatMessageHistory ê°ì²´ ë°˜í™˜

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initail_text=""):
        self.container = container
        self.initail_text = initail_text
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def generate_response(model, tokenizer, prompt_text):
    # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    input_ids = tokenizer.encode(tokenizer.bos_token + prompt_text + tokenizer.eos_token, return_tensors='pt')

    # ëª¨ë¸ ì¶”ë¡ 
    outputs = model.generate(input_ids, max_length=50, repetition_penalty=2.0, num_beams=5, early_stopping=True)
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response_text

if user_input := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    
    # pre-trained modelì„ ì‚¬ìš©í•´ì„œ ë‹µë³€ ìƒì„±
    #  í”„ë¡¶í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ì§ˆë¬¸ì— ê³µê° í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”" ,
            ) ,
            # ëŒ€í™” ê¸°ë¡ì„ í•¨
            MessagesPlaceholder(variable_name="history"),
            ('human',"{question}"), # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ 
        ]
    )
    
    # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ìƒì„±
    session_history = get_session_history("122")
    history_messages = [ChatMessage(role=m.role, content=m.content) for m in session_history.messages]
    full_prompt = prompt.format(history=history_messages, question=user_input)
    
    # ì‘ë‹µ ìƒì„±
    msg = generate_response(trained_model, trained_tokenizer, full_prompt)

    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        st.write(msg)
        st.session_state.messages.append(ChatMessage(role="assistant", content=msg))
        # ì„¸ì…˜ ê¸°ë¡ì— AIì˜ ì‘ë‹µ ì¶”ê°€
        session_history.add_message(ChatMessage(role="assistant", content=msg))
        
    