import streamlit as st
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.callbacks.base import BaseCallbackHandler

st.set_page_config(page_title="ê³µê°ì±—", page_icon="ğŸ’¬",
                   layout="wide", initial_sidebar_state="expanded")
st.title("ğŸ’¬ê³µê° ì±—ë´‡ì´ê±¸ë‘~")

# ì„¸ì…˜ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì„¸ì…˜ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state.store = dict()

# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
trained_model = GPT2LMHeadModel.from_pretrained('./kogpt2-chatbot')
trained_tokenizer = PreTrainedTokenizerFast.from_pretrained('./kogpt2-chatbot')

# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
def print_message():
    if "messages" in st.session_state and len(st.session_state.messages) > 0:
        for chat_message in st.session_state.messages:
            st.chat_message(chat_message.role).write(chat_message.content)
print_message()

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in st.session_state.store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id]  # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ChatMessageHistory ê°ì²´ ë°˜í™˜

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.container.markdown(self.text)
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def stream_generate_response(model, tokenizer, prompt_text, stream_handler=None):
    input_ids = tokenizer.encode(tokenizer.bos_token + prompt_text + tokenizer.eos_token, return_tensors='pt')
    output_ids = input_ids.clone()

    for _ in range(50):  # ìµœëŒ€ í† í° ìˆ˜ ì„¤ì •
        outputs = model(input_ids=output_ids)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        output_ids = torch.cat([output_ids, next_token_id], dim=-1)
        
        next_token = tokenizer.decode(next_token_id[0])
        
        # ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ê°€ ìˆëŠ” ê²½ìš°, í† í°ì„ í•˜ë‚˜ì”© ì—…ë°ì´íŠ¸
        if stream_handler:
            stream_handler.on_llm_new_token(next_token)
        
        # ëë‚˜ëŠ” í† í°ì„ ë§Œë‚¬ì„ ë•Œ ì¤‘ë‹¨
        if next_token_id.item() == tokenizer.eos_token_id:
            break

    response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return response_text

if user_input := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
    st.chat_message("user").write(f"{user_input}")
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ì§ˆë¬¸ì— ê³µê°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”"),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}"),
        ]
    )

    session_history = get_session_history("122")
    history_messages = [ChatMessage(role=m.role, content=m.content) for m in session_history.messages]
    full_prompt = prompt.format(history=history_messages, question=user_input)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        msg = stream_generate_response(trained_model, trained_tokenizer, full_prompt, stream_handler)
        
        st.session_state.messages.append(ChatMessage(role="assistant", content=msg))
        session_history.add_message(ChatMessage(role="assistant", content=msg))
