# ì˜ìƒì„ ì°¸ê³ í•´ì„œ ë§Œë“¤ê¸° ì‹œì‘ https://www.youtube.com/watch?v=ZVmLe3odQvc

import streamlit as st
# from langchain_openai import OpenAI
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.output_parsers import StrOutputParser
# from langchain_openai import ChatOpenAI


st.set_page_config(page_title="ê³µê°ì±—", page_icon="ğŸ’¬",
                   layout="wide", initial_sidebar_state="expanded")
st.title("ğŸ’¬ê³µê° ì±—ë´‡ì´ê±¸ë‘~")

# ì„¸ì…˜ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì„ ì–¸
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì €ì¥ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# KoGPT2 ëª¨ë¸ ë¡œë“œ
trained_model = GPT2LMHeadModel.from_pretrained('./kogpt2-chatbot')
trained_tokenizer = PreTrainedTokenizerFast.from_pretrained('./kogpt2-chatbot')
# trained_model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")
# trained_tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')

# ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ
def print_message():
    if "messages" in st.session_state and len(st.session_state.messages) > 0:
        for chat_message in st.session_state.messages:
            st.chat_message(chat_message.role).write(chat_message.content)
print_message()

store = {} # ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    print(session_id)
    if session_id not in store: # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ê³  storeì— ì €ì¥
        store[session_id] = ChatMessageHistory()
    return store[session_id] # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ChatMessageHistory ê°ì²´ ë°˜í™˜


if user_input := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    
    # pre-trained modelì„ ì‚¬ìš©í•´ì„œ ë‹µë³€ ìƒì„±
    # 1. ëª¨ë¸ ìƒì„±
    llm = trained_model
    
    # 2. í”„ë¡¶í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ì§ˆë¬¸ì— ê³µê° í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”" ,
            ) ,
            # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, historyê°€ MessageHistoryì˜ keyê°€ ë¨
            MessagesPlaceholder(variable_name="history"),
            ('human',"{question}"), # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì…ë ¥ 
        ]
    )
    chain = prompt | llm
    
    chain_with_memory = (
        RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="question",
            history_messages_key="history",
        )
    )
    
    response = chain_with_memory.invoke(
        {"question": user_input},
        # ì„¸ì…˜ID ì„¤ì •
        config={"configurations": {"session_id": "122"}},
    )
    
    msg = response.content
    
    # AIì˜ ë‹µë³€
    with st.chat_message("assistant"):
        st.write(msg)
        st.session_state.messages.append(ChatMessage(role="assistant", content=msg))
        
        
        
    # # ì…ë ¥ ë¬¸ì¥ í† í°í™”
    # # user_input = "ì•ˆë…•"
    # input_ids = trained_tokenizer.encode(trained_tokenizer.bos_token + user_input + trained_tokenizer.eos_token, return_tensors='pt')

    # # ëª¨ë¸ ì¶”ë¡ 
    # outputs = trained_model.generate(input_ids, max_length=50, repetition_penalty=2.0, num_beams=5, early_stopping=True)
    # msg = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)
    # # print(msg)
    