{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "836ILJIsc-oS",
        "outputId": "4cfe93e6-3244-4f91-a9af-d67522d05237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr3jj_8TY5S4",
        "outputId": "4e6ab9cb-3879-4f00-ce73-83d142818eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, peft\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 peft-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import PreTrainedTokenizerFast, AutoModelForSequenceClassification, GPT2LMHeadModel\n",
        "from accelerate import Accelerator\n"
      ],
      "metadata": {
        "id": "IEEmUCpve2Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수\n",
        "# 그 외 추가 가능 인자 https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "\n",
        "threshold=0.55 # float, 0~1\n",
        "max_length = 45 # int, default=20\n",
        "min_new_tokens = 3 # int\n",
        "use_cache = True # bool, default=True\n",
        "repetition_penalty = 4.0 # float, default=1.0\n",
        "do_sample = False # bool, default=False\n",
        "num_beams = 2 # int, default=1\n",
        "temperature = 2.0 # float, default=1.0\n",
        "top_k = 50 # int, default=50\n",
        "top_p = 1.0 # float, default=1.0"
      ],
      "metadata": {
        "id": "D4tVgEfknhuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_LORA_MODEL = False"
      ],
      "metadata": {
        "id": "X-3sOZRdYt0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    inference_mode=True,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")\n",
        "distil_cls_peft_config = LoraConfig(\n",
        "    task_type = \"SEQ_CLS\", #시퀀스 분류 작업을 위한 설정\n",
        "    inference_mode = True, #모델이 추론모드인지 여부 설정\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=['attention.q_lin', 'attention.k_lin', 'attention.v_lin', 'attention.out_lin', 'ffn.lin1', 'ffn.lin2']\n",
        ")\n",
        "\n",
        "gen_peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=True,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\")"
      ],
      "metadata": {
        "id": "gsbgkrGBd5SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_TKN = \"<Q>\"\n",
        "A_TKN = \"<A>\"\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "UNK = '<unk>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<sent>'\n",
        "PAD = '<pad>'"
      ],
      "metadata": {
        "id": "5f5SKs1Gi0n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분류 모델"
      ],
      "metadata": {
        "id": "mvq39FctdbsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. kogpt2"
      ],
      "metadata": {
        "id": "_Mj6rRphl2p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "cls_path = '/content/drive/MyDrive/Colab Notebooks/kogpt2-classification'\n",
        "cls_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    cls_path,\n",
        "    num_labels=5,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "if USE_LORA_MODEL:\n",
        "  accelerator = Accelerator() #데이터 병렬 처리 원활\n",
        "  model = accelerator.prepare(model)\n",
        "  model = get_peft_model(model, peft_config)\n",
        "\n",
        "cls_tokenizer = PreTrainedTokenizerFast.from_pretrained(cls_path,\n",
        "                        bos_token=BOS, eos_token=EOS, unk_token=UNK,\n",
        "                        pad_token=PAD, mask_token=MASK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3QIcNtKdeG8",
        "outputId": "62ef6062-560d-4fd9-a543-85a874ad5fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. distilKoBERT"
      ],
      "metadata": {
        "id": "vckg8rcZl80T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "distil_cls_path = '/content/drive/MyDrive/Colab Notebooks/distilKoBERT-classification'\n",
        "cls_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "      distil_cls_path,\n",
        "      num_labels=5,\n",
        "      problem_type=\"multi_label_classification\",\n",
        "      )\n",
        "accelerator = Accelerator() #데이터 병렬 처리 원활\n",
        "model = accelerator.prepare(model)\n",
        "# lora 적용\n",
        "trained_model = get_peft_model(model, lora_config)\n",
        "\n",
        "cls_tokenizer = BertTokenizerFast.from_pretrained(distil_cls_path)"
      ],
      "metadata": {
        "id": "GrxPUvKol5mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_listener_empathy(input_text, model, tokenizer, threshold=threshold):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 문장 토큰화\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # 모델에 입력을 전달하여 로짓(logits)을 얻음\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 로짓에 시그모이드 적용하여 확률로 변환\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "    # 임계값을 기준으로 이진화\n",
        "    predictions = (probabilities > threshold).int()\n",
        "\n",
        "    # 레이블 디코딩\n",
        "    label_classes = ['조언', '격려', '위로', '동조', '']\n",
        "    num_classes = 5\n",
        "    predicted_labels = [label_classes[i] for i in range(num_classes) if predictions[0][i] == 1]\n",
        "\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "37UV33FfdkIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 생성 모델"
      ],
      "metadata": {
        "id": "94P400-5dXut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 및 토크나이저 로드\n",
        "gen_path = '/content/drive/MyDrive/Colab Notebooks/kogpt2-chatbot'\n",
        "gen_model = GPT2LMHeadModel.from_pretrained(gen_path)\n",
        "if USE_LORA:\n",
        "  accelerator = Accelerator() #데이터 병렬 처리 원활\n",
        "  trained_gen_model = accelerator.prepare(trained_gen_model)\n",
        "  trained_gen_model = get_peft_model(trained_gen_model, gen_peft_config)\n",
        "\n",
        "gen_tokenizer = PreTrainedTokenizerFast.from_pretrained(gen_path,\n",
        "                          bos_token=BOS, eos_token=EOS, unk_token=UNK,\n",
        "                          pad_token=PAD, mask_token=MASK)"
      ],
      "metadata": {
        "id": "A4JdPr3bdDJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(predicted_labels, input_text, model, tokenizer,\n",
        "                   max_length, min_new_tokens, use_cache,\n",
        "                   repetition_penalty, do_sample, num_beams,\n",
        "                   temperature, top_k, top_p):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "    # 입력 문장 토큰화\n",
        "    empathy = ' ,'.join(map(str, predicted_labels))\n",
        "    inputs = Q_TKN + input_text + SENT + empathy + ' ' + A_TKN\n",
        "    input_ids = tokenizer.encode(tokenizer.bos_token + inputs + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # 모델 추론\n",
        "    outputs = model.generate(input_ids,\n",
        "                             max_length=max_length, min_new_tokens=min_new_tokens, use_cache=use_cache,\n",
        "                             repetition_penalty=repetition_penalty, do_sample=do_sample, num_beams=num_beams,\n",
        "                             temperature=temperature, top_k=top_k, top_p=top_p, early_stopping=True)\n",
        "    output_text = trained_gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text = output_text.split(A_TKN)[1]\n",
        "\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "jIw9-2aDdRDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론"
      ],
      "metadata": {
        "id": "4m1mAQhgg2U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 입력 문장\n",
        "input_text = \"나는 오늘 카페에 갈거야\"\n",
        "\n",
        "# 분류 결과 추론\n",
        "# threshold 잘 설정해야\n",
        "predicted_labels = predict_listener_empathy(input_text, cls_model, cls_tokenizer, threshold)\n",
        "print(f\"Predicted labels: {predicted_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57KawhJ1dnER",
        "outputId": "3b3a03c1-76d4-47b1-96b7-747f698a54d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: ['조언', '격려', '동조']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = predict_answer(predicted_labels, input_text, gen_model, gen_tokenizer,\n",
        "                   max_length, min_new_tokens, use_cache,\n",
        "                   repetition_penalty, do_sample, num_beams,\n",
        "                   temperature, top_k, top_p)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKfYiXHggMgG",
        "outputId": "5773c5b6-4007-4812-ec71-4a2b475d2c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#카페 #카페스타그램 #카페투어 #\n"
          ]
        }
      ]
    }
  ]
}